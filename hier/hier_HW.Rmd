---
title: "Homework 7 - Hierarchical models"
author: "your name here"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

# Question 1 - compare the James-Stein estimator to MLE

In `jamesstein.Rmd`, within the context of a particular
hierarchical model, we looked at the sum of squared error for
various values of `B`, where `B=0` indicates the use of $x_i$ alone
as an estimate for $\theta_i$ (the maximum likelihood estimate), and
`B=1` indicates returning 0 as an estimate of $\theta_i$ for all *i*.

We show two particular values for `B`, one we called `eb` which is the
James-Stein estimator, and one we called `oracle` which is the optimal
`B` if we knew the variance of $\theta_i$.

Make a plot that compares the **root mean squared error** (RMSE) for the
following three estimators: the MLE, `eb`, and `oracle`. You should
evaluate the three estimators for the following values of
`sigma.theta` (note that this vector implies you should have 25
iterations for each unique value of `sigma.theta`.

Use `k=30` instead of 100 as we had in `jamesstein.Rmd`. This will
help us tease apart the difference between `eb` and `oracle`, as it
makes it harder to estimate the variance of $\theta_i$.

```{r}
k <- 30
sigmas <- rep(seq(.25,3,by=.25), each=25)
table(sigmas)
```

You can perform the simulations and record the answer however you
like. For some guidance, in the answer key, I've used the paradigm:

```{r eval=FALSE}
res <- t(sapply(sigmas, compareJS))
res <- cbind(sigmas, res)
head(res) # result is a matrix with nrows = length(sigmas)
```

Where `compareJS` is a function that takes in a `sigma.theta` value
and gives back a vector of length 3 (the root mean squared error for the
three different estimators).

You can plot the RMSE however you like, but ideally you would show
the individual RMSE's for each iteration, as well as an average RMSE for
each estimator and each unique `sigma.theta`. In the answer key, I have
code for doing this with base R and with ggplot2. 

How would you describe the differences you see in the method
performance as `sigma.theta` goes from .25 to 3?

# Question 2 - compare limma's posterior variances to sample variances

We saw in `hierarchical.Rmd` how limma moderates its variance
estimates for each gene toward a middle value that is chosen by
looking at the sample variance estimates from all genes. In this
question we will explore how, on a random subset of the samples, the
limma moderated variance estimate compares to the standard sample
variance estimate. We will make the comparison on the scale of
**standard deviation**, and we will consider the sample standard
deviation over the full dataset as the "true" standard deviation.  You
should then calculate the root mean squared error (RMSE) of the square
root of the limma moderated variance on the subset to the "truth" and
of the sample standard deviation on the subset to the "truth".

The following code, from `hierarchical.Rmd` loads the expression data:

```{r message=FALSE}
library(curatedBladderData)
library(affy)
data(GSE13507_eset)
e <- GSE13507_eset
e <- e[,which(e$summarystage == "superficial")]
library(matrixStats)
rm <- rowMeans(exprs(e))
e <- e[rm > 8,]
e <- e[,-which(colnames(e)=="GSM340606")]
```

This is the per-gene sample standard deviation (over 102 samples) that
we will consider as the true standard deviation:

```{r}
rsd <- rowSds(exprs(e))
```

You should use the following vector of sample sizes, *n*, implying
that for each sample size from 3 to 20, you should do 5 random subsets
of the data. For each iteration, you should run limma and obtain the
posterior variance estimate `s2.post`, which you will then take the
square root of. You should also calculate the sample standard deviation.

```{r}
ns <- rep(c(seq(3,20)),each=5)
table(ns)
```

You can plot the RMSE for both methods across the 
iterations however you like, but ideally you would
show the individual RMSEâ€™s for each iteration, as well as an average
RMSE for each estimator and each unique sample size *n*. 
This will be similar code as in the first question.

How would you describe the differences you see in the method
performance as *n* goes from 3 to 20?

